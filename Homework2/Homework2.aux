\relax 
\@writefile{toc}{\contentsline {section}{\numberline {0.1}The Value of a Question}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.1.1}Find the decrease in uncertainty $H(X)-H(X|Y)$}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {0.2}Random Questions}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.1}Show $I(X;Q,A)=H(A|Q)$. Interpret.}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.2.2}Now suppose that two i.i.d. questions $Q_1, Q_2 \nobreakspace  {} r(q)$ are asked, eliciting answers $A_1$ and $A_2$. Show that two questions are less valuable than twice a single question in the sense that $I(X;Q_1,A_1,Q_2,A_2) \leq 2I(X;Q_1,A_1)$.}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {0.3}Entropy of a disjoint mixture}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.1}Find $H(X)$ in terms of $H(X_1)$ and $H(X_2)$ and $\alpha $}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.2}Maximize over $\alpha $ to show that $2^{H(X)} \leq 2^{H(X_1)}+2^{H(X_2)}$ and interpret using the notion that $2^{H(X)}$ is the effective alphabet size.}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.3.3}Let $X_1$ and $X_2$ be uniformly distributed over their alphabets. What is the maximizing $\alpha $ and the associated $H(X)$}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {0.4}Bottleneck}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.1}Show that the dependence of $X_1$ and $X_3$ is limited by the bottleneck by proving that $I(X_1;X_3) \leq log(k)$}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.4.2}Evaluate $I(X_1;X_3)$ for $k=1$, and conclude that no dependence can survive such a bottleneck}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {0.5}Relative entropy is not symmetric}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.1}Calculate $H(p), H(q), D(p||q)$ and $D(q||p)$}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.5.2}Verify that in this case $D(p||q) \not =D(q||p)$}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {0.6}Conditional mutual information}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.6.1}Find the mutual information}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {0.7}Conditional mutual information vs unconditional mutual information}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.7.1}Give examples of joint random variables such that $X, Y,$ and $Z$ such that:}{4}}
\@writefile{toc}{\contentsline {subsubsection}{(a)$(I(X;Y|Z) < I(X;Y)$}{4}}
\@writefile{toc}{\contentsline {subsubsection}{(b)$I(X;Y|Z) > I(X;Y)$}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {0.8}Entropy of initial conditions}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.8.1}Prove that $H(X_0|X_n)$ is non-decreasing with $n$ for any Markov chain}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {0.9}A metric}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.9.1}Show that $\rho (X,Y) = H(X|Y) + H(Y|X)$ satisfies the first, second and fourth properties above. If we say that $X=Y$ if there is a one-to-one function mapping $X$ to $Y$, then the third property is also satisfied, and $\rho (X,Y)$ is a metric}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.9.2}Verify that $\rho (X,Y)$ can also be expressed as}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {0.10}Run length coding}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.10.1}Show all equalities and inequalities, and give simple bounds on all differences}{6}}
\@writefile{toc}{\contentsline {section}{\numberline {0.11}Pure randomness and bent coins}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.11.1}Give reasons for the following inequalities}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {0.11.2}Exhibit a good map f on sequences of length 4}{6}}
